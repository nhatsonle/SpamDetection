{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cbf32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang đọc dữ liệu...\n",
      "Đang áp dụng tiền xử lý văn bản...\n",
      "Tiền xử lý văn bản hoàn tất.\n",
      "5 dòng đầu với Cleaned_Message:\n",
      "                                             Message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                     Cleaned_Message Category  \n",
      "0  go jurong point crazy available bugis n great ...      ham  \n",
      "1                            ok lar joking wif u oni      ham  \n",
      "2  free entry 2 wkly comp win fa cup final tkts 2...     spam  \n",
      "3                u dun say early hor u c already say      ham  \n",
      "4        nah dont think goes usf lives around though      ham  \n"
     ]
    }
   ],
   "source": [
    "# 6. Triển khai Support Vector Machine (SVM)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC # Import lớp SVM cho phân loại\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import sys\n",
    "import os\n",
    "import joblib # Dùng để lưu/tải mô hình\n",
    "\n",
    "# --- Đảm bảo có thể import từ thư mục src ---\n",
    "# Thêm thư mục gốc của dự án vào đường dẫn Python\n",
    "module_path = os.path.abspath(os.path.join('..')) # Giả sử đang chạy từ notebooks hoặc tương tự\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# --- Import hàm tiền xử lý từ src ---\n",
    "try:\n",
    "    # Giả sử hàm tiền xử lý của bạn tên là clean_text_v1 trong file preprocess.py\n",
    "    from src.preprocess import clean_text_v1\n",
    "except ImportError:\n",
    "    print(\"Lỗi: Không thể import hàm clean_text_v1 từ src/preprocess.py.\")\n",
    "    print(\"Hãy đảm bảo file tồn tại và chứa hàm cần thiết.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khác khi import hàm tiền xử lý: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Đọc và Chuẩn bị Dữ liệu ---\n",
    "print(\"Đang đọc dữ liệu...\")\n",
    "try:\n",
    "    # Sử dụng đường dẫn tương đối nếu cần\n",
    "    data_path = '../data/processed/spam_cleaned_columns.csv'\n",
    "    df = pd.read_csv(data_path, encoding='latin-1')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Lỗi: Không tìm thấy file {data_path}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Áp dụng Tiền xử lý ---\n",
    "print(\"Đang áp dụng tiền xử lý văn bản...\")\n",
    "df['Cleaned_Message'] = df['Message'].apply(clean_text_v1)\n",
    "print(\"Tiền xử lý văn bản hoàn tất.\")\n",
    "print(\"5 dòng đầu với Cleaned_Message:\")\n",
    "print(df[['Message', 'Cleaned_Message', 'Category']].head())\n",
    "\n",
    "# --- 3. Chuẩn bị Dữ liệu cho Mô hình ---\n",
    "X = df['Cleaned_Message']\n",
    "y = df['Category_Num']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5fda349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đã chia dữ liệu: 4457 huấn luyện, 1115 kiểm tra\n",
      "Đang thực hiện vector hóa TF-IDF...\n",
      "Đang lấy danh sách tên đặc trưng...\n",
      "Số lượng đặc trưng: 5000\n",
      "\n",
      "Kiểm tra các n-grams quan trọng:\n",
      "'limited time' KHÔNG CÓ trong từ điển.\n",
      "'time offer' KHÔNG CÓ trong từ điển.\n",
      "'exclusive discount' KHÔNG CÓ trong từ điển.\n",
      "'50 percent' KHÔNG CÓ trong từ điển.\n",
      "'percent discount' KHÔNG CÓ trong từ điển.\n",
      "'this friday' KHÔNG CÓ trong từ điển.\n",
      "'friday only' KHÔNG CÓ trong từ điển.\n",
      "'limited' KHÔNG CÓ trong từ điển.\n",
      "'offer' CÓ trong từ điển.\n",
      "'exclusive' KHÔNG CÓ trong từ điển.\n",
      "'discount' CÓ trong từ điển.\n",
      "'percent' KHÔNG CÓ trong từ điển.\n",
      "'friday' CÓ trong từ điển.\n",
      "'only' KHÔNG CÓ trong từ điển.\n",
      "Kích thước ma trận TF-IDF tập huấn luyện: (4457, 5000)\n",
      "Kích thước ma trận TF-IDF tập kiểm tra: (1115, 5000)\n",
      "\n",
      "Bắt đầu huấn luyện mô hình SVM...\n",
      "Đã huấn luyện xong mô hình SVM.\n",
      "\n",
      "Kết quả đánh giá SVM (kernel='linear', C=mặc định):\n",
      "Accuracy: 0.9847533632286996\n",
      "\n",
      "Confusion Matrix:\n",
      " [[960   6]\n",
      " [ 11 138]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       966\n",
      "        spam       0.96      0.93      0.94       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.97      0.96      0.97      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Chia Dữ liệu Train/Test ---\n",
    "# Sử dụng cùng random_state và stratify như khi làm Naive Bayes để đảm bảo tính nhất quán\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#stratify: đảm bảo tỷ lệ của từng lớp trong tập huấn luyện và kiểm tra giống nhau\n",
    "print(f\"\\nĐã chia dữ liệu: {X_train.shape[0]} huấn luyện, {X_test.shape[0]} kiểm tra\")\n",
    "\n",
    "# --- 5. Vector hóa TF-IDF ---\n",
    "# Sử dụng lại cài đặt TF-IDF giống như với Naive Bayes\n",
    "print(\"Đang thực hiện vector hóa TF-IDF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "\n",
    "#fit_transform: tạo ra vector cho tập huấn luyện\n",
    "\n",
    "\n",
    "print(\"Đang lấy danh sách tên đặc trưng...\")\n",
    "try:\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    print(f\"Số lượng đặc trưng: {len(feature_names)}\")\n",
    "    # print(\"Một vài tên đặc trưng đầu tiên:\", feature_names[:20]) # In ra 20 đặc trưng đầu tiên để kiểm tra\n",
    "    \n",
    "    # KIỂM TRA CÁC N-GRAMS QUAN TRỌNG TỪ HAI CÂU SPAM\n",
    "    print(\"\\nKiểm tra các n-grams quan trọng:\")\n",
    "    important_ngrams = [\"limited time\", \"time offer\", \"exclusive discount\", \n",
    "                          \"50 percent\", \"percent discount\", \"this friday\", \"friday only\",\n",
    "                          \"limited\", \"offer\", \"exclusive\", \"discount\", \"percent\", \"friday\", \"only\"] # Thêm cả unigrams\n",
    "    \n",
    "    found_ngrams = []\n",
    "    for ngram in important_ngrams:\n",
    "        if ngram in feature_names:\n",
    "            found_ngrams.append(ngram)\n",
    "            print(f\"'{ngram}' CÓ trong từ điển.\")\n",
    "        else:\n",
    "            print(f\"'{ngram}' KHÔNG CÓ trong từ điển.\")\n",
    "    \n",
    "    if not found_ngrams:\n",
    "        print(\"Không tìm thấy n-gram quan trọng nào trong từ điển với max_features=5000 và ngram_range=(1,2).\")\n",
    "        print(\"Hãy thử tăng max_features hoặc sử dụng min_df thay thế.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi lấy tên đặc trưng: {e}\")\n",
    "\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "#transform: tạo ra vector cho tập kiểm tra\n",
    "#fit_transform gồm 2 bước fit và transform\n",
    "#fit: tạo ra vectorizer (bộ từ vựng từ tất cả các từ trong tập huấn luyện)\n",
    "#transform: tạo ra vector cho tập huấn luyện và kiểm tra\n",
    "#Không sử dụng fit_transform để tránh rò rỉ dữ liệu test cho mô hình, chỉ sử dụng fit để tạo ra vectorizer\n",
    "\n",
    "\n",
    "print(f\"Kích thước ma trận TF-IDF tập huấn luyện: {X_train_tfidf.shape}\")\n",
    "print(f\"Kích thước ma trận TF-IDF tập kiểm tra: {X_test_tfidf.shape}\")\n",
    "\n",
    "# --- 6. Huấn luyện Mô hình SVM ---\n",
    "print(\"\\nBắt đầu huấn luyện mô hình SVM...\")\n",
    "# Khởi tạo mô hình SVM (SVC - Support Vector Classifier)\n",
    "# kernel='linear': Thường hiệu quả cho dữ liệu văn bản.\n",
    "# probability=True: Cho phép sử dụng predict_proba sau này (có thể làm chậm huấn luyện một chút).\n",
    "# random_state=42: Để đảm bảo kết quả có thể lặp lại (nếu thuật toán có yếu tố ngẫu nhiên).\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "print(\"Đã huấn luyện xong mô hình SVM.\")\n",
    "\n",
    "#    Giả sử có 3 từ trong từ điển: \"free\", \"prize\", \"meeting\"\n",
    "   \n",
    "#    Spam messages:\n",
    "#    - \"Get free prize now!\"     -> [0.8, 0.7, 0.0]\n",
    "#    - \"Win free prize today!\"   -> [0.7, 0.8, 0.0]\n",
    "#    - \"Claim your free prize!\"  -> [0.9, 0.6, 0.0]\n",
    "   \n",
    "#    Ham messages:\n",
    "#    - \"Meeting at 2pm\"          -> [0.0, 0.0, 0.9]\n",
    "#    - \"Schedule meeting\"        -> [0.0, 0.0, 0.8]\n",
    "#    - \"Cancel meeting\"          -> [0.0, 0.0, 0.7]\n",
    "\n",
    "#Các vector spam tập trung ở 1 vùng (có nhiều từ \"free\", \"prize\")\n",
    "#Các vector ham tập trung ở 1 vùng (có nhiều từ \"meeting\")\n",
    "#Dễ dàng tìm được siêu phẳng phân tíchtích\n",
    "\n",
    "# --- 7. Dự đoán trên Tập Kiểm tra ---\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# --- 8. Đánh giá Mô hình ---\n",
    "print(\"\\nKết quả đánh giá SVM (kernel='linear', C=mặc định):\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "# target_names giúp hiển thị tên lớp 'ham', 'spam' thay vì 0, 1\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_svm, target_names=['ham', 'spam']))\n",
    "\n",
    "# --- (Tùy chọn) Lưu mô hình và vectorizer ---\n",
    "# print(\"\\nĐang lưu mô hình và vectorizer...\")\n",
    "# output_dir = '../results/trained_models/'\n",
    "# os.makedirs(output_dir, exist_ok=True) # Tạo thư mục nếu chưa có\n",
    "# joblib.dump(svm_model, os.path.join(output_dir, 'svm_linear_model.pkl'))\n",
    "# joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.pkl'))\n",
    "# print(\"Đã lưu mô hình và vectorizer vào thư mục results/trained_models/\")\n",
    "\n",
    "# --- (Tùy chọn) Lấy xác suất dự đoán ---\n",
    "# y_pred_proba_svm = svm_model.predict_proba(X_test_tfidf)\n",
    "# print(\"\\nXác suất dự đoán cho 5 mẫu đầu tiên:\\n\", y_pred_proba_svm[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a07a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đang lưu mô hình và vectorizer...\n",
      "Đã lưu mô hình và vectorizer vào thư mục results/trained_models/\n"
     ]
    }
   ],
   "source": [
    "# --- (Tùy chọn) Lưu mô hình và vectorizer ---\n",
    "print(\"\\nĐang lưu mô hình và vectorizer...\")\n",
    "output_dir = '../results/trained_models/'\n",
    "os.makedirs(output_dir, exist_ok=True) # Tạo thư mục nếu chưa có\n",
    "joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.pkl'))\n",
    "print(\"Đã lưu mô hình và vectorizer vào thư mục results/trained_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce6a4699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu mô hình SVM tốt nhất.\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../results/trained_models/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(svm_model, os.path.join(output_dir, 'svm_model.pkl'))\n",
    "print(\"Đã lưu mô hình SVM tốt nhất.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6547d4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bắt đầu Test Case ---\n",
      "Số lượng tin nhắn thử nghiệm: 6\n",
      "\n",
      "Tin nhắn sau khi tiền xử lý:\n",
      "1: urgent claim free 1000 prize click httpspamlinkcom\n",
      "2: hey wondering youre free coffee later today\n",
      "3: meet singles area text date 88888 tcs apply 18\n",
      "4: remember buy bread eggs way back\n",
      "5: limited time offer exclusive discount\n",
      "6: 50 percent discount friday\n",
      "\n",
      "Đã vector hóa 6 tin nhắn thành ma trận TF-IDF kích thước: (6, 5000)\n",
      "\n",
      "Đang thực hiện dự đoán bằng mô hình SVM...\n",
      "\n",
      "--- Kết quả Dự đoán Test Case ---\n",
      "\n",
      "Tin nhắn gốc : \"URGENT! Claim your FREE £1000 prize now! Click http://spamlink.com\"\n",
      "  -> Dự đoán   : SPAM\n",
      "  -> Xác suất : [Ham=0.0063, Spam=0.9937]\n",
      "\n",
      "Tin nhắn gốc : \"Hey, wondering if you're free for coffee later today?\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9988, Spam=0.0012]\n",
      "\n",
      "Tin nhắn gốc : \"Meet singles in your area, text DATE to 88888 T&Cs apply 18+\"\n",
      "  -> Dự đoán   : SPAM\n",
      "  -> Xác suất : [Ham=0.0204, Spam=0.9796]\n",
      "\n",
      "Tin nhắn gốc : \"Remember to buy bread and eggs on your way back.\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9994, Spam=0.0006]\n",
      "\n",
      "Tin nhắn gốc : \"Limited time offer! Exclusive discount just for you!\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9034, Spam=0.0966]\n",
      "\n",
      "Tin nhắn gốc : \"50 percent discount this Friday only!\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9760, Spam=0.0240]\n",
      "\n",
      "--- Hoàn thành Test Case ---\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Cell mới: Tạo Test Case và Dự đoán bằng mô hình SVM đã huấn luyện\n",
    "\n",
    "# --- Giả định các biến sau đã tồn tại từ cell trước ---\n",
    "# svm_model: Đối tượng mô hình SVC đã được .fit()\n",
    "# tfidf_vectorizer: Đối tượng TfidfVectorizer đã được .fit_transform() trên tập train\n",
    "# clean_text_v1: Hàm tiền xử lý văn bản đã được định nghĩa hoặc import\n",
    "\n",
    "# --- Định nghĩa các tin nhắn mới cần dự đoán ---\n",
    "new_messages_for_test = [\n",
    "    \"URGENT! Claim your FREE £1000 prize now! Click http://spamlink.com\", # Tin nhắn có vẻ là SPAM\n",
    "    \"Hey, wondering if you're free for coffee later today?\", # Tin nhắn có vẻ là HAM\n",
    "    \"Meet singles in your area, text DATE to 88888 T&Cs apply 18+\", # Tin nhắn có vẻ là SPAM\n",
    "    \"Remember to buy bread and eggs on your way back.\", # Tin nhắn có vẻ là HAM\n",
    "    \"Limited time offer! Exclusive discount just for you!\", # Tin nhắn có vẻ là SPAM\n",
    "    \"50 percent discount this Friday only!\" #SPAM\n",
    "]\n",
    "\n",
    "print(\"--- Bắt đầu Test Case ---\")\n",
    "print(f\"Số lượng tin nhắn thử nghiệm: {len(new_messages_for_test)}\")\n",
    "\n",
    "# --- 1. Tiền xử lý các tin nhắn mới ---\n",
    "# Áp dụng hàm tiền xử lý đã có (cần đảm bảo hàm clean_text_v1 đã được định nghĩa/import ở cell trước)\n",
    "try:\n",
    "    cleaned_test_messages = [clean_text_v1(msg) for msg in new_messages_for_test]\n",
    "    print(\"\\nTin nhắn sau khi tiền xử lý:\")\n",
    "    for i, msg in enumerate(cleaned_test_messages):\n",
    "        print(f\"{i+1}: {msg}\")\n",
    "except NameError:\n",
    "    print(\"\\nLỗi: Hàm 'clean_text_v1' chưa được định nghĩa hoặc import.\")\n",
    "    print(\"Vui lòng chạy cell định nghĩa/import hàm này trước.\")\n",
    "    # Dừng thực thi cell này nếu hàm chưa có\n",
    "    raise\n",
    "\n",
    "# --- 2. Vector hóa các tin nhắn mới ---\n",
    "# *** Quan trọng: Dùng đối tượng tfidf_vectorizer đã fit từ cell trước, chỉ gọi .transform() ***\n",
    "try:\n",
    "    new_messages_tfidf_test = tfidf_vectorizer.transform(cleaned_test_messages)\n",
    "    print(f\"\\nĐã vector hóa {len(cleaned_test_messages)} tin nhắn thành ma trận TF-IDF kích thước: {new_messages_tfidf_test.shape}\")\n",
    "except NameError:\n",
    "    print(\"\\nLỗi: Biến 'tfidf_vectorizer' chưa được định nghĩa.\")\n",
    "    print(\"Vui lòng chạy cell huấn luyện TF-IDF Vectorizer trước.\")\n",
    "    raise\n",
    "\n",
    "# --- 3. Dự đoán ---\n",
    "print(\"\\nĐang thực hiện dự đoán bằng mô hình SVM...\")\n",
    "try:\n",
    "    # Dự đoán nhãn (0 hoặc 1)\n",
    "    predictions_test = svm_model.predict(new_messages_tfidf_test)\n",
    "    # Dự đoán xác suất (P(ham), P(spam))\n",
    "    # Lưu ý: svm_model phải được khởi tạo với probability=True ở cell huấn luyện\n",
    "    probabilities_test = svm_model.predict_proba(new_messages_tfidf_test)\n",
    "except NameError:\n",
    "    print(\"\\nLỗi: Biến 'svm_model' chưa được định nghĩa.\")\n",
    "    print(\"Vui lòng chạy cell huấn luyện mô hình SVM trước.\")\n",
    "    raise\n",
    "except AttributeError:\n",
    "     print(\"\\nLỗi: Không thể lấy xác suất.\")\n",
    "     print(\"Để dùng predict_proba(), mô hình SVM cần được khởi tạo với tham số 'probability=True'.\")\n",
    "     # Gán giá trị mặc định để code không bị lỗi tiếp\n",
    "     probabilities_test = np.array([[0.5, 0.5]] * len(predictions_test)) # Tạo mảng xác suất giả định\n",
    "\n",
    "# --- 4. Hiển thị kết quả ---\n",
    "print(\"\\n--- Kết quả Dự đoán Test Case ---\")\n",
    "label_map = {0: 'ham', 1: 'spam'} # Ánh xạ nhãn số về chữ\n",
    "\n",
    "for i, original_message in enumerate(new_messages_for_test):\n",
    "    predicted_label_num = predictions_test[i]\n",
    "    predicted_label_text = label_map[predicted_label_num]\n",
    "    prob_ham = probabilities_test[i][0] # Xác suất là ham (lớp 0)\n",
    "    prob_spam = probabilities_test[i][1] # Xác suất là spam (lớp 1)\n",
    "\n",
    "    print(f\"\\nTin nhắn gốc : \\\"{original_message}\\\"\")\n",
    "    # print(f\"Tin nhắn sạch: \\\"{cleaned_test_messages[i]}\\\"\") # Bỏ comment nếu muốn xem\n",
    "    print(f\"  -> Dự đoán   : {predicted_label_text.upper()}\")\n",
    "    # Chỉ in xác suất nếu lấy được\n",
    "    if 'AttributeError' not in locals() or not isinstance(locals()['AttributeError'], AttributeError):\n",
    "         print(f\"  -> Xác suất : [Ham={prob_ham:.4f}, Spam={prob_spam:.4f}]\")\n",
    "\n",
    "print(\"\\n--- Hoàn thành Test Case ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7d23fa",
   "metadata": {},
   "source": [
    "B1:Tạo 1 file google doc -> Note lại toàn bộ kết quả baseline của từng thuật toán (accuracy, precision, recall, f1-score, confusion matrix)\n",
    "B2: Tinh chỉnh siêu tham số\n",
    "- random forest\n",
    "- Logistic Regression\n",
    "B3: Sử dụng GridSearchCV hoặc RandomizedSearchCV (CV: Cross validation)\n",
    "- Cross Validation: K-Fold Cross Validation (Kiểm định chéo K lần)\n",
    "  + Chia dữ liệu: Chia ngẫu nhiên tập train thành K phần, giá trị K thường là 5 hoặc 10\n",
    "  + Lặp K lần: Quá trình huấn luyện và đánh giá dược lặp lại K lần. Trong mỗi lần lặp (ví dụ lần lặp thứ i):\n",
    "    > Chọn Fold kiểm tra: Fold thứ i được giữ lại làm tập dữ liệu kiểm định (validation set) hoặc là \"test fold\" cho lần lặp này.\n",
    "    >Chọn fold huấn luyện: K-1 phần còn lại được gộp lại để làm tập dữ liệu huấn luyện.\n",
    "    >Huấn luyện: Mô hình học máy được huấn luyện trên tập huấn luyện (k-1 folds)\n",
    "    > Đánh giá: Mô hình vừa huấn luyện xong được đánh giá hiệu năng (tính f1-score, accuracy,..) trên tập kiểm định (fold thứ i). Kết quả đánh giá được lưu lại.\n",
    "  + Tổng hợp kết quả: Sau khi thực hiện K lần lặp thì có K kết quả đánh giá. Hiệu năng cuối cùng của mô hình theo phương pháp CV thường được tính bằng lấy trung bình của K kết quả.\n",
    "- GridSearchCV: thử tất cả các tổ hợp\n",
    "- RandomizedSearchCV: thử một số tổ hợp tham số ngẫu nhiên.\n",
    "\n",
    "B4:Cập nhật kết quả: ghi lại chỉ số hiệu năng của mô hình đã tune vào bảng so sánh chung (file google doc ở trên).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbc4cc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu tinh chỉnh siêu tham số cho SVM\n",
      "Running GridSearchCV on SVM...\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   2.3s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   2.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 3. Fit GridSearchCV into training data\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRunning GridSearchCV on SVM...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mgrid_search_svm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 4. Take out the best parameters and scores\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTham số tốt nhất tìm được cho SVM:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1018\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1019\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1020\u001b[39m     )\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1027\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1028\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1570\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1571\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    963\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    965\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    966\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    967\u001b[39m         )\n\u001b[32m    968\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    990\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    993\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1916\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1917\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1921\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1922\u001b[39m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1923\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1924\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1847\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1849\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:866\u001b[39m, in \u001b[36m_fit_and_score\u001b[39m\u001b[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[39m\n\u001b[32m    864\u001b[39m         estimator.fit(X_train, **fit_params)\n\u001b[32m    865\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m         \u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[32m    870\u001b[39m     fit_time = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:258\u001b[39m, in \u001b[36mBaseLibSVM.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[LibSVM]\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    257\u001b[39m seed = rnd.randint(np.iinfo(\u001b[33m\"\u001b[39m\u001b[33mi\u001b[39m\u001b[33m\"\u001b[39m).max)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[38;5;28mself\u001b[39m.shape_fit_ = X.shape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:377\u001b[39m, in \u001b[36mBaseLibSVM._sparse_fit\u001b[39m\u001b[34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[39m\n\u001b[32m    363\u001b[39m kernel_type = \u001b[38;5;28mself\u001b[39m._sparse_kernels.index(kernel)\n\u001b[32m    365\u001b[39m libsvm_sparse.set_verbosity_wrap(\u001b[38;5;28mself\u001b[39m.verbose)\n\u001b[32m    367\u001b[39m (\n\u001b[32m    368\u001b[39m     \u001b[38;5;28mself\u001b[39m.support_,\n\u001b[32m    369\u001b[39m     \u001b[38;5;28mself\u001b[39m.support_vectors_,\n\u001b[32m    370\u001b[39m     dual_coef_data,\n\u001b[32m    371\u001b[39m     \u001b[38;5;28mself\u001b[39m.intercept_,\n\u001b[32m    372\u001b[39m     \u001b[38;5;28mself\u001b[39m._n_support,\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m._probA,\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m._probB,\n\u001b[32m    375\u001b[39m     \u001b[38;5;28mself\u001b[39m.fit_status_,\n\u001b[32m    376\u001b[39m     \u001b[38;5;28mself\u001b[39m._num_iter,\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m ) = \u001b[43mlibsvm_sparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlibsvm_sparse_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclass_weight_\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshrinking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprobability\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28mself\u001b[39m._warn_from_fit_status()\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mclasses_\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_libsvm_sparse.pyx:218\u001b[39m, in \u001b[36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\2024.2\\CapstoneProject\\AI\\venv\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:29\u001b[39m, in \u001b[36m_cs_matrix.__init__\u001b[39m\u001b[34m(self, arg1, shape, dtype, copy, maxprint)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_cs_matrix\u001b[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001b[32m     25\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    base array/matrix class for compressed row- and column-oriented arrays/matrices\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg1, shape=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m, *, maxprint=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     30\u001b[39m         _data_matrix.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg1, maxprint=maxprint)\n\u001b[32m     32\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m issparse(arg1):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "print(\"Bắt đầu tinh chỉnh siêu tham số cho SVM\")\n",
    "\n",
    "# 1. Define parameters grid\n",
    "param_grid_svm = {\n",
    "  'C': [0.1, 1, 10, 100], # Regularization parameter\n",
    "  'kernel': ['linear', 'rbf'], #linear kernel and rbf\n",
    "  'gamma': ['scale', 'auto', 0.1, 1] #parameters for kernel RBF (skipped if kernel is linear)\n",
    "  \n",
    "}\n",
    "\n",
    "# 2. Init GridSearchCV\n",
    "grid_search_svm = GridSearchCV(estimator=SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "                               param_grid=param_grid_svm,\n",
    "                               cv = 5,\n",
    "                               scoring= 'recall',\n",
    "                            \n",
    "                               verbose=2,\n",
    "                               )\n",
    "\n",
    "# 3. Fit GridSearchCV into training data\n",
    "print(\"Running GridSearchCV on SVM...\")\n",
    "grid_search_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 4. Take out the best parameters and scores\n",
    "print(\"\\nTham số tốt nhất tìm được cho SVM:\")\n",
    "print(grid_search_svm.best_params_)\n",
    "print(f\"Điểm F1 (cross-validation) tốt nhất cho SVM: {grid_search_svm.best_score_:.4f}\")\n",
    "\n",
    "# 5. Lấy mô hình SVM tốt nhất\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "\n",
    "# 6. Đánh giá mô hình SVM tốt nhất trên tập Test\n",
    "print(\"\\nĐánh giá mô hình SVM tốt nhất trên tập Test:\")\n",
    "y_pred_svm_tuned = best_svm_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm_tuned))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm_tuned))\n",
    "print(\"\\nClassification Report (SVM Tuned):\\n\", classification_report(y_test, y_pred_svm_tuned, target_names=['ham', 'spam']))\n",
    "\n",
    "# (Tùy chọn) Lưu mô hình SVM tốt nhất\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ec21e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu mô hình SVM tốt nhất.\n"
     ]
    }
   ],
   "source": [
    "output_dir = '../results/trained_models/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "joblib.dump(best_svm_model, os.path.join(output_dir, 'best_svm_model.pkl'))\n",
    "print(\"Đã lưu mô hình SVM tốt nhất.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5a6f7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Bắt đầu Test Case ---\n",
      "Số lượng tin nhắn thử nghiệm: 6\n",
      "\n",
      "Tin nhắn sau khi tiền xử lý:\n",
      "1: urgent claim free prize click httpspamlinkcom\n",
      "2: hey wondering youre free coffee later today\n",
      "3: meet singles area text date tcs apply\n",
      "4: remember buy bread eggs way back\n",
      "5: limited time offer exclusive discount\n",
      "6: percent discount friday\n",
      "\n",
      "Đã vector hóa 6 tin nhắn thành ma trận TF-IDF kích thước: (6, 5000)\n",
      "\n",
      "Đang thực hiện dự đoán bằng mô hình SVM...\n",
      "\n",
      "--- Kết quả Dự đoán Test Case ---\n",
      "\n",
      "Tin nhắn gốc : \"URGENT! Claim your FREE £1000 prize now! Click http://spamlink.com\"\n",
      "  -> Dự đoán   : SPAM\n",
      "  -> Xác suất : [Ham=0.0000, Spam=1.0000]\n",
      "\n",
      "Tin nhắn gốc : \"Hey, wondering if you're free for coffee later today?\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9993, Spam=0.0007]\n",
      "\n",
      "Tin nhắn gốc : \"Meet singles in your area, text DATE to 88888 T&Cs apply 18+\"\n",
      "  -> Dự đoán   : SPAM\n",
      "  -> Xác suất : [Ham=0.0076, Spam=0.9924]\n",
      "\n",
      "Tin nhắn gốc : \"Remember to buy bread and eggs on your way back.\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9990, Spam=0.0010]\n",
      "\n",
      "Tin nhắn gốc : \"Limited time offer! Exclusive discount just for you!\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9934, Spam=0.0066]\n",
      "\n",
      "Tin nhắn gốc : \"50 percent discount this Friday only!\"\n",
      "  -> Dự đoán   : HAM\n",
      "  -> Xác suất : [Ham=0.9858, Spam=0.0142]\n",
      "\n",
      "--- Hoàn thành Test Case ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_messages_for_test = [\n",
    "    \"URGENT! Claim your FREE £1000 prize now! Click http://spamlink.com\", # Tin nhắn có vẻ là SPAM\n",
    "    \"Hey, wondering if you're free for coffee later today?\", # Tin nhắn có vẻ là HAM\n",
    "    \"Meet singles in your area, text DATE to 88888 T&Cs apply 18+\", # Tin nhắn có vẻ là SPAM\n",
    "    \"Remember to buy bread and eggs on your way back.\", # Tin nhắn có vẻ là HAM\n",
    "    \"Limited time offer! Exclusive discount just for you!\", # Tin nhắn có vẻ là SPAM\n",
    "    \"50 percent discount this Friday only!\"\n",
    "]\n",
    "\n",
    "print(\"--- Bắt đầu Test Case ---\")\n",
    "print(f\"Số lượng tin nhắn thử nghiệm: {len(new_messages_for_test)}\")\n",
    "\n",
    "# --- 1. Tiền xử lý các tin nhắn mới ---\n",
    "# Áp dụng hàm tiền xử lý đã có (cần đảm bảo hàm clean_text_v1 đã được định nghĩa/import ở cell trước)\n",
    "try:\n",
    "    cleaned_test_messages = [clean_text_v1(msg) for msg in new_messages_for_test]\n",
    "    print(\"\\nTin nhắn sau khi tiền xử lý:\")\n",
    "    for i, msg in enumerate(cleaned_test_messages):\n",
    "        print(f\"{i+1}: {msg}\")\n",
    "except NameError:\n",
    "    print(\"\\nLỗi: Hàm 'clean_text_v1' chưa được định nghĩa hoặc import.\")\n",
    "    print(\"Vui lòng chạy cell định nghĩa/import hàm này trước.\")\n",
    "    # Dừng thực thi cell này nếu hàm chưa có\n",
    "    raise\n",
    "\n",
    "# --- 2. Vector hóa các tin nhắn mới ---\n",
    "# *** Quan trọng: Dùng đối tượng tfidf_vectorizer đã fit từ cell trước, chỉ gọi .transform() ***\n",
    "try:\n",
    "    new_messages_tfidf_test = tfidf_vectorizer.transform(cleaned_test_messages)\n",
    "    print(f\"\\nĐã vector hóa {len(cleaned_test_messages)} tin nhắn thành ma trận TF-IDF kích thước: {new_messages_tfidf_test.shape}\")\n",
    "except NameError:\n",
    "    print(\"\\nLỗi: Biến 'tfidf_vectorizer' chưa được định nghĩa.\")\n",
    "    print(\"Vui lòng chạy cell huấn luyện TF-IDF Vectorizer trước.\")\n",
    "    raise\n",
    "\n",
    "# --- 3. Dự đoán ---\n",
    "print(\"\\nĐang thực hiện dự đoán bằng mô hình SVM...\")\n",
    "try:\n",
    "    # Dự đoán nhãn (0 hoặc 1)\n",
    "    predictions_test = svm_model.predict(new_messages_tfidf_test)\n",
    "    # Dự đoán xác suất (P(ham), P(spam))\n",
    "    # Lưu ý: svm_model phải được khởi tạo với probability=True ở cell huấn luyện\n",
    "    probabilities_test = best_svm_model.predict_proba(new_messages_tfidf_test)\n",
    "except NameError:\n",
    "    print(\"\\nLỗi: Biến 'svm_model' chưa được định nghĩa.\")\n",
    "    print(\"Vui lòng chạy cell huấn luyện mô hình SVM trước.\")\n",
    "    raise\n",
    "except AttributeError:\n",
    "     print(\"\\nLỗi: Không thể lấy xác suất.\")\n",
    "     print(\"Để dùng predict_proba(), mô hình SVM cần được khởi tạo với tham số 'probability=True'.\")\n",
    "     # Gán giá trị mặc định để code không bị lỗi tiếp\n",
    "     probabilities_test = np.array([[0.5, 0.5]] * len(predictions_test)) # Tạo mảng xác suất giả định\n",
    "\n",
    "# --- 4. Hiển thị kết quả ---\n",
    "print(\"\\n--- Kết quả Dự đoán Test Case ---\")\n",
    "label_map = {0: 'ham', 1: 'spam'} # Ánh xạ nhãn số về chữ\n",
    "\n",
    "for i, original_message in enumerate(new_messages_for_test):\n",
    "    predicted_label_num = predictions_test[i]\n",
    "    predicted_label_text = label_map[predicted_label_num]\n",
    "    prob_ham = probabilities_test[i][0] # Xác suất là ham (lớp 0)\n",
    "    prob_spam = probabilities_test[i][1] # Xác suất là spam (lớp 1)\n",
    "\n",
    "    print(f\"\\nTin nhắn gốc : \\\"{original_message}\\\"\")\n",
    "    # print(f\"Tin nhắn sạch: \\\"{cleaned_test_messages[i]}\\\"\") # Bỏ comment nếu muốn xem\n",
    "    print(f\"  -> Dự đoán   : {predicted_label_text.upper()}\")\n",
    "    # Chỉ in xác suất nếu lấy được\n",
    "    if 'AttributeError' not in locals() or not isinstance(locals()['AttributeError'], AttributeError):\n",
    "         print(f\"  -> Xác suất : [Ham={prob_ham:.4f}, Spam={prob_spam:.4f}]\")\n",
    "\n",
    "print(\"\\n--- Hoàn thành Test Case ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6987eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 3 stored elements and shape (1, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 1108)\t0.5298416056073167\n",
      "  (0, 1527)\t0.5237108275551885\n",
      "  (0, 3536)\t0.6670793371623199\n",
      "percent: 0.6670793371623199\n",
      "discount: 0.5298416056073167\n",
      "friday: 0.5237108275551885\n"
     ]
    }
   ],
   "source": [
    "vec = tfidf_vectorizer.transform([\"percent discount friday\"])\n",
    "print(vec)\n",
    "# Hoặc kiểm tra giá trị cụ thể của từng từ\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for word in [\"percent\", \"discount\", \"friday\"]:\n",
    "    if word in tfidf_vectorizer.vocabulary_:\n",
    "        idx = tfidf_vectorizer.vocabulary_[word]\n",
    "        print(f\"{word}: {vec[0, idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ea76f",
   "metadata": {},
   "source": [
    "Trong NLP, có rất nhiều cách để vector hoá 1 văn bản text thành ma trận dạng số: Bag_of_words, TF-IDF, word embeddings,..\n",
    "- Bag_of_words: \"I arrive home at 6 o'clock\" => [1,1,1,1]\n",
    "\"I come back home and then I go to my grandpa's house\" \n",
    "- TF-IDF: Term Frequency-Inverse Document Frequency\n",
    "Term Frequency (Bag_of_words) * Inverse Document Frequency \n",
    "IDF = log(number of documents / number of documents in which this word exists)\n",
    "\n",
    "VD: 'Tôi đi học'\n",
    "'Tôi đi chơi'\n",
    "'Mua đồ ăn'\n",
    "\n",
    "IDF = log(3 / 2) = 0.58 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
